{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Forecasting Pipeline - Synthetic Data Example\n",
                "\n",
                "This notebook presents a complete example of a **forecasting pipeline** using synthetic data.  \n",
                "It covers the entire workflow, from data preparation and preprocessing to model training, prediction generation, and performance evaluation.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "1.  **Libraries and Synthetic Data Generation:**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "name": "Librerías"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Synthetic data generated:\n",
                        "        Date  Product_ID  Quantity_Sold\n",
                        "0 2021-01-01  prod_reg_0         105.75\n",
                        "1 2021-02-01  prod_reg_0          96.15\n",
                        "2 2021-03-01  prod_reg_0          98.60\n",
                        "3 2021-04-01  prod_reg_0          98.79\n",
                        "4 2021-05-01  prod_reg_0         110.82\n"
                    ]
                }
            ],
            "source": [
                "# Data manipulation and analysis\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "# Visualization\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Example of a machine learning algorithm (clustering)\n",
                "# Note: we will use it later in the pipeline\n",
                "from sklearn.cluster import KMeans\n",
                "\n",
                "# To suppress unnecessary warnings for cleaner output\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# ---------------------------------------------------------\n",
                "# Generate Synthetic Data\n",
                "# ---------------------------------------------------------\n",
                "\n",
                "# Create a range of monthly dates from January 2021 to December 2023\n",
                "dates = pd.date_range(start='2021-01-01', end='2023-12-01', freq='MS')\n",
                "\n",
                "# Define the total number of products to simulate\n",
                "n_products = 10000\n",
                "\n",
                "# Split products into two categories:\n",
                "# - Regular demand (70% of products)\n",
                "# - Intermittent demand (30% of products)\n",
                "regular_products = [f'prod_reg_{i}' for i in range(int(n_products * 0.7))]\n",
                "intermittent_products = [f'prod_int_{i}' for i in range(int(n_products * 0.3))]\n",
                "\n",
                "# Initialize an empty DataFrame that will contain all synthetic records\n",
                "df_synthetic = pd.DataFrame()\n",
                "\n",
                "# ---------------------------------------------------------\n",
                "# Generate Regular Demand Products\n",
                "# ---------------------------------------------------------\n",
                "# For regular products, demand follows a normal distribution\n",
                "# with mean=100 and std=20. Negative values are clipped to 1.\n",
                "for prod in regular_products:\n",
                "    demand = np.random.normal(loc=100, scale=20, size=len(dates)).round(2).clip(min=1)\n",
                "    df_temp = pd.DataFrame({\n",
                "        'Date': dates,\n",
                "        'Product_ID': prod,\n",
                "        'Quantity_Sold': demand\n",
                "    })\n",
                "    # Concatenate with the main dataset\n",
                "    df_synthetic = pd.concat([df_synthetic, df_temp])\n",
                "\n",
                "# ---------------------------------------------------------\n",
                "# Generate Intermittent Demand Products\n",
                "# ---------------------------------------------------------\n",
                "# For intermittent products, most months have zero demand.\n",
                "# A random number of months (between 3 and 10) will contain sales,\n",
                "# with quantities randomly chosen between 10 and 500 units.\n",
                "for prod in intermittent_products:\n",
                "    demand = np.zeros(len(dates))  # initialize with zero demand\n",
                "    sale_indices = np.random.choice(len(dates), size=np.random.randint(3, 10), replace=False)\n",
                "    demand[sale_indices] = np.random.randint(low=10, high=500, size=len(sale_indices))\n",
                "    df_temp = pd.DataFrame({\n",
                "        'Date': dates,\n",
                "        'Product_ID': prod,\n",
                "        'Quantity_Sold': demand\n",
                "    })\n",
                "    df_synthetic = pd.concat([df_synthetic, df_temp])\n",
                "\n",
                "# ---------------------------------------------------------\n",
                "# Preview of the generated synthetic dataset\n",
                "# ---------------------------------------------------------\n",
                "print(\"Synthetic data generated:\")\n",
                "print(df_synthetic.head())\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "2.  **Data Preparation and Characterization:**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "collapsed": false,
                "name": "Gr?fico del c?do para cl?sters"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Features calculated for clustering:\n",
                        "      Product_ID  Total_Volume  Num_Periods_Sold       ADI       CV2\n",
                        "0     prod_int_0        1189.0                 4  4.664915  0.190690\n",
                        "1     prod_int_1        1306.0                 5  3.244087  0.280302\n",
                        "2    prod_int_10        1591.0                 5  3.244087  0.269070\n",
                        "3   prod_int_100        1534.0                 7  5.502628  0.105215\n",
                        "4  prod_int_1000        1839.0                 8  4.003191  0.345730\n"
                    ]
                }
            ],
            "source": [
                "# Filter out zero-demand records to focus only on periods with sales\n",
                "sales_only_df = df_synthetic[df_synthetic['Quantity_Sold'] > 0].copy()\n",
                "\n",
                "# ---------------------------------------------------------\n",
                "# Aggregate features at the product level\n",
                "# ---------------------------------------------------------\n",
                "# - Total_Volume: total units sold across all periods\n",
                "# - Num_Periods_Sold: number of periods in which the product had sales\n",
                "features_df = sales_only_df.groupby('Product_ID').agg(\n",
                "    Total_Volume=('Quantity_Sold', 'sum'),\n",
                "    Num_Periods_Sold=('Date', 'count')\n",
                ").reset_index()\n",
                "\n",
                "# ---------------------------------------------------------\n",
                "# Function to calculate intermittency metrics (ADI and CV²)\n",
                "# ---------------------------------------------------------\n",
                "def calculate_metrics(group):\n",
                "    \"\"\"\n",
                "    Calculate intermittency metrics for a product's sales history.\n",
                "\n",
                "    ADI (Average Demand Interval): average time gap between sales events.\n",
                "    CV² (Squared Coefficient of Variation): measures demand variability.\n",
                "    \"\"\"\n",
                "    if len(group) < 2:\n",
                "        # If there are fewer than 2 sales events:\n",
                "        # - ADI is estimated as the span of time / months + 1\n",
                "        # - CV² is set to 1.0 (maximum variability by default)\n",
                "        adi = (group['Date'].max() - group['Date'].min()).days / 30.44 + 1 if len(group) > 0 else len(dates) + 1\n",
                "        cv2 = 1.0\n",
                "    else:\n",
                "        # Sort sales chronologically\n",
                "        group = group.sort_values('Date')\n",
                "\n",
                "        # Compute time gaps between sales in months\n",
                "        demand_intervals = group['Date'].diff().dt.days / 30.44\n",
                "        adi = demand_intervals.mean()\n",
                "\n",
                "        # Compute squared coefficient of variation of demand\n",
                "        mean_demand = group['Quantity_Sold'].mean()\n",
                "        std_dev_demand = group['Quantity_Sold'].std()\n",
                "        cv2 = (std_dev_demand / mean_demand)**2 if mean_demand > 0 and std_dev_demand > 0 else 0.0\n",
                "    \n",
                "    return pd.Series({'ADI': adi, 'CV2': cv2})\n",
                "\n",
                "# ---------------------------------------------------------\n",
                "# Apply intermittency metrics to each product\n",
                "# ---------------------------------------------------------\n",
                "intermittency_df = sales_only_df.groupby('Product_ID').apply(calculate_metrics).reset_index()\n",
                "\n",
                "# ---------------------------------------------------------\n",
                "# Merge all product-level features into a single dataset\n",
                "# ---------------------------------------------------------\n",
                "features_df = pd.merge(features_df, intermittency_df, on='Product_ID', how='left')\n",
                "\n",
                "# Fill missing values with default assumptions:\n",
                "# - Products with no sales get maximum ADI and CV² = 1.0\n",
                "features_df.fillna({\n",
                "    'Total_Volume': 0,\n",
                "    'Num_Periods_Sold': 0,\n",
                "    'ADI': len(dates) + 1,\n",
                "    'CV2': 1.0\n",
                "}, inplace=True)\n",
                "\n",
                "# ---------------------------------------------------------\n",
                "# Preview calculated features\n",
                "# ---------------------------------------------------------\n",
                "print(\"\\nFeatures calculated for clustering:\")\n",
                "print(features_df.head())\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "3.  **Clustering for Demand Segmentation:**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Segmentation results:\n",
                        "Segment\n",
                        "Regular         7437\n",
                        "Intermittent    2563\n",
                        "Name: count, dtype: int64\n",
                        "\n",
                        "Average profile of each segment:\n",
                        "              Total_Volume  Num_Periods_Sold   ADI   CV2\n",
                        "Segment                                                 \n",
                        "Intermittent       1459.87              5.74  6.23  0.37\n",
                        "Regular            3498.72             34.31  1.13  0.06\n"
                    ]
                }
            ],
            "source": [
                "# Prepare the feature set for clustering\n",
                "# ---------------------------------------------------------\n",
                "# We use ADI (Average Demand Interval) and CV² (Squared Coefficient of Variation)\n",
                "# as inputs to cluster products based on their demand patterns.\n",
                "X = features_df[['ADI', 'CV2']]\n",
                "\n",
                "# ---------------------------------------------------------\n",
                "# Train the K-Means clustering model\n",
                "# ---------------------------------------------------------\n",
                "# We set the number of clusters = 2 to separate \"Regular\" vs \"Intermittent\" products.\n",
                "kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
                "features_df['Segment_ID'] = kmeans.fit_predict(X)\n",
                "\n",
                "# ---------------------------------------------------------\n",
                "# Assign descriptive labels to each cluster\n",
                "# ---------------------------------------------------------\n",
                "# We compare average number of periods sold across clusters.\n",
                "# - The cluster with higher frequency of sales is labeled \"Regular\"\n",
                "# - The other cluster is labeled \"Intermittent\"\n",
                "segments = features_df.groupby('Segment_ID')[['ADI', 'Num_Periods_Sold']].mean()\n",
                "if segments.iloc[0]['Num_Periods_Sold'] > segments.iloc[1]['Num_Periods_Sold']:\n",
                "    mapping = {segments.index[0]: 'Regular', segments.index[1]: 'Intermittent'}\n",
                "else:\n",
                "    mapping = {segments.index[0]: 'Intermittent', segments.index[1]: 'Regular'}\n",
                "\n",
                "# Apply the mapping\n",
                "features_df['Segment'] = features_df['Segment_ID'].map(mapping)\n",
                "\n",
                "# ---------------------------------------------------------\n",
                "# Display segmentation results\n",
                "# ---------------------------------------------------------\n",
                "print(\"\\nSegmentation results:\")\n",
                "print(features_df['Segment'].value_counts())\n",
                "\n",
                "# ---------------------------------------------------------\n",
                "# Profile the segments (average characteristics)\n",
                "# ---------------------------------------------------------\n",
                "segments_profile = features_df.groupby('Segment')[['Total_Volume', 'Num_Periods_Sold', 'ADI', 'CV2']].mean().round(2)\n",
                "print(\"\\nAverage profile of each segment:\")\n",
                "print(segments_profile)\n",
                "\n",
                "# ---------------------------------------------------------\n",
                "# Merge product segmentation results back into the main dataset\n",
                "# ---------------------------------------------------------\n",
                "df_final = pd.merge(df_synthetic, features_df[['Product_ID', 'Segment']], on='Product_ID', how='left')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "4. **Forecasting Models for Each Segment:**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "collapsed": false,
                "name": "Tabla con datos finales"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- 4. Forecast for Regular Products (ESM Simulation) ---\n",
                        "\n",
                        "      Product_ID  Regular_Forecast\n",
                        "0     prod_int_1         36.277778\n",
                        "1    prod_int_10         44.194444\n",
                        "2  prod_int_1014         60.972222\n",
                        "3  prod_int_1024         57.694444\n",
                        "4  prod_int_1035         37.361111\n",
                        "\n",
                        "--- 5. Forecast for Intermittent Products (Croston) ---\n",
                        "\n",
                        "      Product_ID  Intermittent_Forecast\n",
                        "0     prod_int_0              49.827391\n",
                        "1   prod_int_100              50.165191\n",
                        "2  prod_int_1000              52.692604\n",
                        "3  prod_int_1001              58.904294\n",
                        "4  prod_int_1002              29.529824\n"
                    ]
                }
            ],
            "source": [
                "# Split the dataset by demand segment\n",
                "df_regular = df_final[df_final['Segment'] == 'Regular']\n",
                "df_intermittent = df_final[df_final['Segment'] == 'Intermittent']\n",
                "\n",
                "# ---------------------------------------------------------\n",
                "# Forecast for Regular Segment (Simulation of Exponential Smoothing)\n",
                "# ---------------------------------------------------------\n",
                "def forecast_regular(group):\n",
                "    \"\"\"\n",
                "    Simple forecasting function for regular-demand products.\n",
                "    For demonstration purposes, we use the historical average\n",
                "    as a naive forecast (simulating Exponential Smoothing behavior).\n",
                "    \"\"\"\n",
                "    forecast_value = group['Quantity_Sold'].mean()\n",
                "    return pd.Series({'Regular_Forecast': forecast_value})\n",
                "\n",
                "# Apply the forecasting function for each regular product\n",
                "regular_forecasts = df_regular.groupby('Product_ID').apply(forecast_regular).reset_index()\n",
                "\n",
                "print(\"\\n--- 4. Forecast for Regular Products (ESM Simulation) ---\\n\")\n",
                "print(regular_forecasts.head())\n",
                "\n",
                "# ---------------------------------------------------------\n",
                "# Forecast for Intermittent Segment (Croston's Method)\n",
                "# ---------------------------------------------------------\n",
                "def forecast_croston(group):\n",
                "    \"\"\"\n",
                "    Implementation of Croston's method for intermittent demand.\n",
                "    Steps:\n",
                "    - Filter non-zero sales.\n",
                "    - Calculate intervals between demand occurrences.\n",
                "    - Apply exponential smoothing separately to demand size and intervals.\n",
                "    - Final forecast = smoothed demand / smoothed interval.\n",
                "    \"\"\"\n",
                "    # Ensure 'Date' is the index for interval calculations\n",
                "    demand_series = group.set_index('Date')['Quantity_Sold']\n",
                "    demand = demand_series[demand_series > 0]  # keep only non-zero demand\n",
                "    \n",
                "    # Handle edge cases with insufficient demand history\n",
                "    if len(demand) < 2:\n",
                "        return np.mean(demand) if len(demand) > 0 else 0\n",
                "    \n",
                "    # Calculate intervals (in months) between demand occurrences\n",
                "    intervals = demand.index.to_series().diff().dt.days / 30.44\n",
                "    \n",
                "    # Initialize smoothing parameters\n",
                "    alpha = 0.2\n",
                "    demand_smooth = [demand.iloc[0]]\n",
                "    interval_smooth = [intervals.iloc[1]]\n",
                "    \n",
                "    # Recursive exponential smoothing for demand and intervals\n",
                "    for i in range(1, len(demand)):\n",
                "        demand_smooth.append(alpha * demand.iloc[i] + (1 - alpha) * demand_smooth[-1])\n",
                "        interval_smooth.append(alpha * intervals.iloc[i] + (1 - alpha) * interval_smooth[-1])\n",
                "        \n",
                "    # Final smoothed values\n",
                "    final_demand = demand_smooth[-1]\n",
                "    final_interval = interval_smooth[-1]\n",
                "    \n",
                "    # Croston forecast\n",
                "    return final_demand / final_interval if final_interval > 0 else 0\n",
                "\n",
                "# Apply Croston's method to intermittent products\n",
                "intermittent_forecasts = df_intermittent.groupby('Product_ID').apply(forecast_croston).reset_index(name='Intermittent_Forecast')\n",
                "\n",
                "print(\"\\n--- 5. Forecast for Intermittent Products (Croston) ---\\n\")\n",
                "print(intermittent_forecasts.head())\n"
            ]
        }
    ],
    "metadata": {
        "authors": [
            {
                "name": "AI Assistant"
            }
        ],
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.2"
        },
        "title": "FULL_FORECAST_PIPELINE"
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
